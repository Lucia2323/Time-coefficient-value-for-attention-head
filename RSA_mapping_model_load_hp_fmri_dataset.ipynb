{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RSA mapping model: load_hp_fmri_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucia2323/Time-coefficient-value-for-attention-head/blob/main/RSA_mapping_model_load_hp_fmri_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhUl2kx7x6Ye"
      },
      "source": [
        "WHEN CLOSURE, SAVE IT ON GITHUB\n",
        "\n",
        "To run this colab, you first need to upload the harry potter dataset into your google drive and give colab access to my Drive.\n",
        "\n",
        "Note: In placed a link to the HP_data folder in my own drive, and replaced all occurrences of 'HPfmri/' in the code with 'HP_data/fMRI/', I got a warning about being connected to a GPU runtime, and wasting it. I don't know how important that is - perhaps better to save GPU time for when we need it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FijfxKnFYMu"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJjAa0mMIZi2",
        "outputId": "6dfc80ae-33a7-4ea6-844d-14aac4405aee"
      },
      "source": [
        "#@title Mount google drive (where the data is uploaded).\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImCSjS9VKyWY"
      },
      "source": [
        "subject_id = 'K' #@param: so the subjects are parameters\n",
        "subject_data = np.load(f'/content/drive/My Drive/HP_data/fMRI/data_subject_{subject_id}.npy')\n",
        "time_fmri = np.load('/content/drive/My Drive/HP_data/fMRI/time_fmri.npy')\n",
        "runs_fmri = np.load('/content/drive/My Drive/HP_data/fMRI/runs_fmri.npy') #a trial? \n",
        "words_fmri = np.load('/content/drive/My Drive/HP_data/fMRI/words_fmri.npy')\n",
        "time_words_fmri = np.load('/content/drive/My Drive/HP_data/fMRI/time_words_fmri.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvTkAP2ORHVJ"
      },
      "source": [
        "#delay = time in seconds between the first of the four words is shown to the subject and the fMRI scan is taken ->\n",
        "#match each four-words to its scan by a timelime: to estimate in seconds the time between each four words-scan \n",
        "#(standard brain data preprocessing procedure)\n",
        "def delay_one(mat, d): \n",
        "  \"\"\"delays a matrix by a delay d. Positive d ==> row t (row at the time the scan was taken) has row t-d.\"\"\"\n",
        "  new_mat = np.zeros_like(mat) #Return an array of zeros with the same shape and type as a given array.\n",
        "  if d>0:\n",
        "      new_mat[d:] = mat[:-d]\n",
        "  elif d<0:\n",
        "      new_mat[:d] = mat[-d:]\n",
        "  else:\n",
        "      new_mat = mat\n",
        "  return new_mat\n",
        "\n",
        "\n",
        "def delay_mat(mat, delays):\n",
        "  \"\"\"delays a matrix by a set of delays d.\n",
        "    a row t in the returned matrix has the concatenated:\n",
        "    row(t-delays[0],t-delays[1]...t-delays[last] ).\n",
        "  \"\"\"\n",
        "  new_mat = np.concatenate([delay_one(mat, d) for d in delays],axis = -1)\n",
        "  return new_mat\n",
        "\n",
        "\n",
        "#we look at similarities between the model and brain representations for the words until EOS (end of a sentence) \n",
        "#token is reached       \n",
        "def prepare_nlp_features(train_features, test_features, word_train_indicator, TR_train_indicator, SKIP_WORDS=20, END_WORDS=5176):\n",
        "    time = np.load('/content/drive/My Drive/HP_data/fMRI/time_fmri.npy')\n",
        "    runs = np.load('/content/drive/My Drive/HP_data/fMRI/runs_fmri.npy') \n",
        "    time_words = np.load('/content/drive/My Drive/HP_data/fMRI/time_words_fmri.npy')\n",
        "    time_words = time_words[SKIP_WORDS:END_WORDS]\n",
        "    #'SKIP_WORDS'?\n",
        "        \n",
        "    words_id = np.zeros([len(time_words)])\n",
        "    # w=find what TR (trial?) each word belongs to\n",
        "    for i in range(len(time_words)):\n",
        "        words_id[i] = np.where(time_words[i]> time)[0][-1]\n",
        "        \n",
        "    all_features = np.zeros([time_words.shape[0], train_features.shape[1]])\n",
        "    all_features[word_train_indicator] = train_features\n",
        "    all_features[~word_train_indicator] = test_features\n",
        "        \n",
        "    p = all_features.shape[1]\n",
        "    tmp = np.zeros([time.shape[0], p])\n",
        "    for i in range(time.shape[0]):\n",
        "        tmp[i] = np.mean(all_features[(words_id<=i)*(words_id>i-1)],0)\n",
        "    tmp = delay_mat(tmp, np.arange(1,5))\n",
        "\n",
        "    # remove the edges of each run(?)\n",
        "    tmp = np.vstack([zscore(tmp[runs==i][20:-15]) for i in range(1,5)])\n",
        "    tmp = np.nan_to_num(tmp)\n",
        "        \n",
        "    return tmp[TR_train_indicator], tmp[~TR_train_indicator]\n",
        "\n",
        "\n",
        "#K-fold cross-validation? \n",
        "def get_fold_flags(n, n_folds):\n",
        "    flags = np.zeros((n))\n",
        "    num_items_in_each_fold = int(np.floor(n/n_folds))\n",
        "    for i in range(0,n_folds -1):\n",
        "        flags[i*num_items_in_each_fold:(i+1)*num_items_in_each_fold] = i\n",
        "    flags[(n_folds-1)*num_items_in_each_fold:] = (n_folds-1)\n",
        "    return flags\n",
        "\n",
        "def tr_to_word_train_indicator(tr_train_indicator, skip_words=20, end_words=5176):\n",
        "    time = np.load('/content/drive/My Drive/HP_data/fMRI/time_fmri.npy')\n",
        "    runs = np.load('/content/drive/My Drive/HP_data/fMRI/runs_fmri.npy') \n",
        "    time_words = np.load('/content/drive/My Drive/HP_data/fMRI/time_words_fmri.npy')\n",
        "    time_words = time_words[skip_words:end_words]\n",
        "        \n",
        "    word_train_indicator = np.zeros([len(time_words)], dtype=bool)    \n",
        "    words_id = np.zeros([len(time_words)],dtype=int)\n",
        "    # Find what TR each word belongs to.\n",
        "    for i in range(len(time_words)):                \n",
        "        words_id[i] = np.where(time_words[i]> time)[0][-1]\n",
        "        \n",
        "        if words_id[i] <= len(runs) - 15:\n",
        "            offset = runs[int(words_id[i])]*20 + (runs[int(words_id[i])]-1)*15\n",
        "            if tr_train_indicator[int(words_id[i])-offset-1] == 1:\n",
        "                word_train_indicator[i] = True\n",
        "    return word_train_indicator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GbCOo85SBrY",
        "outputId": "2b8d4219-62ce-4267-d836-6bbed545122a"
      },
      "source": [
        "#a brain scan every four words\n",
        "#time notation: datapoints of the Harry Potter and brain scan datasets respect chronological order\n",
        "# - each word from the Harry Potter dataset has a number which corresponds to its chronological order within the text/dataset\n",
        "# - each fMRI scan from the brain dataset has a number which corresponds to its chronological order among the recordings\n",
        "# --> every four words (their numbers) are matched with the number of the corresponding brain scan\n",
        "n_words = subject_data.shape[0]\n",
        "n_voxels = subject_data.shape[1]\n",
        "print('Number of voxels for this subject is {n_voxels} for {n_words} scans.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of voxels for this subject is {n_voxels} for {n_words} scans.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Y1GlCcoY9T"
      },
      "source": [
        "n_folds = 4\n",
        "test_fold = 1\n",
        "skip = 5\n",
        "fold_flags = get_fold_flags(n_words, n_folds=n_folds)\n",
        "train_ind = fold_flags!=test_fold\n",
        "test_ind = fold_flags==test_fold\n",
        "train_indicator = tr_to_word_train_indicator(train_ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm9vQZMisj--",
        "outputId": "d11bc6a8-f5e8-4634-bab0-af225c79a806"
      },
      "source": [
        "#@title Get NLP features (currently it is just a random matrix(?))\n",
        "SKIP_WORDS = 20\n",
        "END_WORDS = 5176\n",
        "\n",
        "# You need to replace this with the embeddings that you get from any NLP model for words_fmri.\n",
        "nlp_features = np.random.uniform(size=(len(words_fmri), 100))\n",
        "\n",
        "\n",
        "train_nlp_features = nlp_features[SKIP_WORDS:END_WORDS,:][train_indicator]\n",
        "test_nlp_features = nlp_features[SKIP_WORDS:END_WORDS,:][~train_indicator]\n",
        "train_features, test_features = prepare_nlp_features(train_nlp_features, test_nlp_features, train_indicator, train_ind)\n",
        "\n",
        "print(train_features.shape, test_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(909, 400) (302, 400)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
            "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "992l0uQYxpEp"
      },
      "source": [
        "#@title Get fmri data points.\n",
        "\n",
        "train_data = subject_data[train_ind]\n",
        "test_data = subject_data[test_ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8JR-czTpFG0"
      },
      "source": [
        "#@title Filter out some of data points in the margins of test and train sets.\n",
        "\n",
        "# skip TRs between train and test data\n",
        "if test_fold == 0: # just remove from front end\n",
        "    train_data = train_data[skip:,:]\n",
        "    train_features = train_features[skip:,:]\n",
        "elif test_fold == n_folds-1: # just remove from back end\n",
        "    train_data = train_data[:-skip,:]\n",
        "    train_features = train_features[:-skip,:]\n",
        "else:\n",
        "    test_data = test_data[skip:-skip,:]\n",
        "    test_features = test_features[skip:-skip,:]\n",
        "\n",
        "# normalize data\n",
        "train_data = np.nan_to_num(zscore(np.nan_to_num(train_data)))\n",
        "test_data = np.nan_to_num(zscore(np.nan_to_num(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvFi-uqVqH2L",
        "outputId": "c82f670a-6434-4091-c303-84c72593dcd0"
      },
      "source": [
        "print('fmri features:', train_data.shape, test_data.shape)\n",
        "print('nlp features:', train_features.shape, test_features.shape)\n",
        "#why first dimensions are the same?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fmri features: (909, 25003) (292, 25003)\n",
            "nlp features: (909, 400) (292, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IUN0vbLFcaH"
      },
      "source": [
        "# Representational Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgeqyk003I2"
      },
      "source": [
        "def dot_product_rsa(reps1, reps2):\n",
        "  \"\"\"Compute representational similarity between two sets of representations.\n",
        "\n",
        "  Args:\n",
        "    reps1: float array; [num_examples, feature_size1]\n",
        "    reps2: float array; [num_examples, feature_size2]\n",
        "  \"\"\"\n",
        "  assert reps1.shape[0] == reps2.shape[0], 'First dimensions of inputs should match.'\n",
        "\n",
        "  # Normalize input representations\n",
        "  reps1 = reps1 / np.linalg.norm(reps1)\n",
        "  reps2 = reps2 / np.linalg.norm(reps2)\n",
        "\n",
        "  # Compute and normalize similarity matrices:\n",
        "  similarities1 = reps1.dot(reps1.T) #instead of dot product you can use any similarity measure. e.g. matrix product a*b=a*bT\n",
        "  similarities2 = reps2.dot(reps2.T) #dot is relative so has to be standardized - what about Pearson r?\n",
        "  similarities1 = similarities1 / np.linalg.norm(similarities1)\n",
        "  similarities2 = similarities2 / np.linalg.norm(similarities2)\n",
        "\n",
        "  # Here you can do pearson-r instead of this (or again any measure of similarity).\n",
        "  similarity_of_similarity = np.sum(similarities1 * similarities2, axis=-1)\n",
        "  return np.sum(similarity_of_similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWa2CjELB5fl"
      },
      "source": [
        "def dot_product_rsa_for_lists_of_reps(reps):\n",
        "  \"\"\"Compute representational similarity between two sets of representations.\n",
        "\n",
        "  Args:\n",
        "    reps: list of float arrays; List of arrrays with shape [num_examples, feature_size]\n",
        "      (feature size can be different for each item in the list).\n",
        "  \"\"\"\n",
        "  norm_reps = []\n",
        "  for rep in reps:\n",
        "  # Normalize input representations\n",
        "    norm_reps.append(rep / np.linalg.norm(rep))\n",
        "\n",
        "  # Compute and normalize similarity matrices:\n",
        "  similarity_matrices = []\n",
        "  for rep in norm_reps:\n",
        "    similarities = rep.dot(rep.T) # instead of dot product you can use any similarity measure\n",
        "    similarities = similarities / np.linalg.norm(similarities)\n",
        "    similarity_matrices.append(similarities)\n",
        "\n",
        "  # Shape of similarity_matrices: [num_of_rep_spaces, num_examples, num_examples]\n",
        "  similarity_matrices = np.asarray(similarity_matrices) \n",
        "  import jax.numpy as jnp\n",
        "  sim_of_sim_mat = jnp.zeros((similarity_matrices.shape[0], \n",
        "                              similarity_matrices.shape[0]))\n",
        "  for i in np.arange(sim_of_sim_mat.shape[0]):\n",
        "    for j in np.arange(sim_of_sim_mat.shape[1]):\n",
        "      # Here you can do pearson-r instead of this (or again any measure of similarity).\n",
        "      similarity_of_similarity = np.sum(\n",
        "          similarity_matrices[i] * similarity_matrices[j], axis=-1)\n",
        "      similarity_score = np.sum(similarity_of_similarity)\n",
        "      sim_of_sim_mat[i][j] = similarity_score\n",
        "  return sim_of_sim_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iOyeZqY1Ox6",
        "outputId": "fcf0d96c-e5a8-4ce1-e4ac-27f55dc00e15"
      },
      "source": [
        "dot_product_rsa(train_data, train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000009"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV3KoUZfDmoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "71845d09-1dc7-4c30-97e1-64d062d75166"
      },
      "source": [
        "dot_product_rsa_for_lists_of_reps([train_data, train_data])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9a01e130608f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdot_product_rsa_for_lists_of_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-691bcd03dc92>\u001b[0m in \u001b[0;36mdot_product_rsa_for_lists_of_reps\u001b[0;34m(reps)\u001b[0m\n\u001b[1;32m     29\u001b[0m           similarity_matrices[i] * similarity_matrices[j], axis=-1)\n\u001b[1;32m     30\u001b[0m       \u001b[0msimilarity_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_of_similarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0msim_of_sim_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msim_of_sim_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   5378\u001b[0m          \u001b[0;34m\"immutable; perhaps you want jax.ops.index_update or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5379\u001b[0m          \"jax.ops.index_add instead?\")\n\u001b[0;32m-> 5380\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5382\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_operator_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ROzWo2D1SWB",
        "outputId": "d7e4b710-a4c5-44f3-bef1-c2306157aa80"
      },
      "source": [
        "dot_product_rsa(train_data, train_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05214565708629563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVsTeJBY3-oB",
        "outputId": "7d445eed-94be-47fd-ab5f-0f1e335604c6"
      },
      "source": [
        "dot_product_rsa(train_features, train_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000009"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTJ69fH14qyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100a13ec-8728-4c67-d0b9-d80c2ab96514"
      },
      "source": [
        "#@title Compare two subjects:\n",
        "\n",
        "subject_id_1 = 'K' #@param \n",
        "subject_data_1 = np.load(f'/content/drive/My Drive/HP_data/fMRI/data_subject_{subject_id_1}.npy')\n",
        "\n",
        "subject_id_2 = 'F' #@param \n",
        "subject_data_2 = np.load(f'/content/drive/My Drive/HP_data/fMRI/data_subject_{subject_id_2}.npy')\n",
        "\n",
        "subject_data_1 = np.nan_to_num(zscore(np.nan_to_num(subject_data_1)))\n",
        "subject_data_2 = np.nan_to_num(zscore(np.nan_to_num(subject_data_2)))\n",
        "\n",
        "print(f'Representational similarity between subject {subject_id_1} and {subject_id_2} is:', dot_product_rsa(subject_data_1, subject_data_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Representational similarity between subject K and F is: 0.012922335085487204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40yyt-26-Yhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}